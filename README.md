    B -->|Streaming| C[PySpark Streaming]
    B -->|Sink| D[MongoDB (Raw Lake)]
    C -->|Aggregations| E[Console / BigQuery]
    D -->|Batch| F[Airflow (Daily Stats)]
    D -->|Real-time| G[Streamlit Dashboard]
```

## ğŸ›  PrÃ©requis

*   **Docker Desktop** (avec au moins 8GB de RAM allouÃ©s).
*   **Python 3.9+**.
*   **Git**.

## ğŸš€ DÃ©marrage Rapide

### 1. Lancer l'Infrastructure
DÃ©marrez Kafka, Zookeeper, Schema Registry, Spark, Airflow et Mongo.

```bash
docker-compose up -d
```
*Attendez quelques minutes que tous les services soient "Healthy".*

### 2. Installer les DÃ©pendances Python
```bash
pip install -r requirements.txt
```

### 3. Lancer les Producteurs (Ingestion)
Dans un terminal :
```bash
# GÃ©nÃ©rer du trafic utilisateur simulÃ©
python src/ingestion/main.py
```
Dans un autre terminal (optionnel) :
```bash
# Ã‰couter les changements Wikimedia en temps rÃ©el
python src/ingestion/wikimedia_producer.py
```

### 4. Lancer le Stockage (Consumer)
Pour sauvegarder les donnÃ©es brutes dans MongoDB :
```bash
python src/storage/mongo_consumer.py
```

### 5. Lancer le Dashboard (Visualisation)
```bash
streamlit run src/visualization/dashboard.py
```
AccÃ©dez Ã  **http://localhost:8501**.

---

## ğŸ“¦ Modules du Projet

### Module 1 : Infrastructure
*   Fichier : `docker-compose.yml`
*   Services : Kafka (Confluent 7.5), Spark (3.5), Airflow (2.8), MongoDB (6.0).

### Module 2 : Ingestion
*   Code : `src/ingestion/`
*   Features : Producteurs Kafka robustes, SÃ©rialisation Avro, Gestion du Backpressure.

### Module 3 : Stockage
*   Code : `src/storage/`
*   Features : MongoDB Sink (Idempotent), BigQuery DDL (`config/bigquery/`).

### Module 4 : Streaming (PySpark)
*   Code : `src/streaming/`
*   Features : Structured Streaming, Watermarking (10min), Windowing (1min).
*   **Run** :
    ```bash
    docker exec -it spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 src/streaming/jobs/process_user_activity.py
    ```

### Module 5 : FiabilitÃ©
*   Features : Checkpointing (`/tmp/checkpoints`), Fault Tolerance.

### Module 6 : Orchestration (Airflow)
*   Code : `dags/`, `src/batch/`
*   UI : **http://localhost:8082** (user: airflow, pass: airflow).
*   Features : DAG quotidien pour le calcul de statistiques.

### Module 7 : Visualisation
*   Code : `src/visualization/`
*   Features : Dashboard Streamlit temps rÃ©el connectÃ© Ã  MongoDB.

---

---

## ğŸ–¥ï¸ Monitoring & Pilotage

AccÃ©dez aux interfaces de contrÃ´le de la plateforme :

| Outil | URL / AccÃ¨s | UtilitÃ© |
| :--- | :--- | :--- |
| **Airflow** | [http://localhost:8082](http://localhost:8082) (airflow/airflow) | Orchestration des DAGs & Batchs |
| **BigQuery** | [Console GCP](https://console.cloud.google.com/bigquery?project=effidic-stage-2026) | Data Warehouse & RequÃªtes SQL |
| **Streamlit** | [http://localhost:8501](http://localhost:8501) | Dashboard Temps RÃ©el |
| **Spark UI** | [http://localhost:9090](http://localhost:9090) | Monitoring des jobs Spark |
| **Kafka UI** | [http://localhost:9021](http://localhost:9021) | Gestion des topics & Schema Registry |

---

## âœ… VÃ©rification du Projet

Pour vÃ©rifier que tout fonctionne correctement :

1.  **Logs du Consumer** : VÃ©rifiez que les messages sont insÃ©rÃ©s dans BigQuery.
    ```bash
    docker logs -f bigquery_consumer (ou via votre terminal)
    ```
2.  **AperÃ§u BigQuery** : Allez dans la console GCP > `monitoring_datalake` > `user_activity` > Onglet **AperÃ§u**.
3.  **Statut dbt** : VÃ©rifiez les tables transformÃ©es dans BigQuery (`fct_daily_user_metrics`).

---

## ğŸ“š Documentation AvancÃ©e

Pour approfondir les aspects techniques et l'industrialisation :

*   ğŸ“– **[Concepts & Notions](docs/COURS_ET_NOTIONS.md)** : Lambda vs Kappa, Avro, Schema Registry.
*   ğŸš€ **[Rapport d'Industrialisation](docs/INDUSTRIALIZATION.md)** : SÃ©curitÃ©, CI/CD, FinOps.
*   ğŸ“Š **[Guide QlikSense](docs/QLIKSENSE_SETUP.md)** : Connexion BI Ã  BigQuery.
*   â˜ï¸ **[Setup BigQuery](docs/README_BIGQUERY.md)** : DÃ©tails de configuration GCP.

---

## ğŸ”§ Troubleshooting Rapide

*   **Docker Error `npipe://...`** : Docker Desktop n'est pas lancÃ©.
*   **403 Forbidden (BigQuery)** : VÃ©rifiez que la facturation est activÃ©e sur GCP.
*   **Kafka Connection Refused** : VÃ©rifiez que les conteneurs sont UP (`docker-compose ps`).

